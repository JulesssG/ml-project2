\documentclass[10pt,conference,compsocconf]{IEEEtran}

\usepackage{hyperref}
\usepackage{graphicx}	% For figure environment

\begin{document}
\title{Earthquake Detection from Seismological Data}

\author{
  Lucien MichaÃ«l Iseli, Florian Maxime Charles Ravasi and Jules Eliot Gottraux\\
  \textit{Master of Data Science, EPFL, Switzerland}
}

\maketitle
\section{Introduction}
In seismology, the detection of earthquakes is a pretty active research. The detection of severe earthquake, those that make your house tremble and are actually dangerous, is not a particularly tricky task but when talking about earthquakes of small magnitude that's a different story. In fact, as of today the detection and classification of earthquakes is still done by hand by specialists. That is they manually inspect the measurements taken by sensors to produce the catalogs, the tables containing the earthquakes's informations. This process is error-prone and tedious, this is why we try to address this issue in this project. We'll use data from sensors that capture the vibration present in the earth, the same data that specialists in detection use, to create a machine learning model able to detect if an earthquake has happened. Most of the work we will present will be prior to the creation and optimization of the machine learning algorithm. That is we spent most of our time preparing the data and doing featuring engineering. As we will see, this task revealed to be very difficult and one would need more time and substantial expertise to obtain satisfying performances for small earthquake detection. However by simplifying the task we managed to get acceptable result. In fact this work is inspired by a research \cite{ConvNetPaper} conducted by three researchers, they worked hard to achieve that using neural networks and clever memory management.

\section{Dataset Characteristics}
We get our data using the ObsPy library \cite{obspy}. ObsPy grants us access to stations, which have sensors, which record the seismological data over time on different channels. Those sensors record  constantly the activity, or vibration, of the earth at their location. This include seismic waves which are the waves of energy released when a seism occur nearby. This data is thus one dimensional and filled with noise as it is a completely raw measure, it is a giant time serie containing the amplitude of waves captured by the sensor. It has the information of the seism and earthquake but this information is aggregated with all the vibration happening near the sensor. The goal will be to create, from this time series, meaningful features for the model containing sufficient information to classify a certain window of time. We'll thus fix the duration of a time window, for example one minute, then compute features on this time window and feed these features to the machine learning model.

\begin{figure}[h]
  \centering
	\includegraphics[width=\columnwidth]{10min-example-2018-06-30T17:50.png}
  \caption{10 minute of data example, no earthquake}
	\label{fig:10min-example}
\end{figure}

So the basis for the features is this time series, for the labels we use a hand-made catalog of earthquake that contains, among other properties, the location, magnitude and time of the earthquake.

Before diving into the machine learning model construction, we have to take care of several difficulties inherent in the dataset.\newline
First the data collection, after having chosen the location, station and channel from which we want to get the time series we have to download it and store that to a usable format for the next steps. The frequency of the sensors is $100$ data point per second, so the amount of data increases quite quickly. Then, the data has holes in it, since it is sometimes missing for some period. Those holes are not regular and unpredictable, so we have to take care of them with caution.

\begin{figure}[h]
  \centering
	\includegraphics[width=\columnwidth]{hole_example.png}
  \caption{Example of missing values in the dataset}
	\label{fig:10min-example}
\end{figure}

We already mentionned that the data obtained from ObsPy is very noisy, we want to emphasize this and as we will see in examples, some earthquakes and non-earthquakes are undistinguishable for the untrained eye. The catalog is noisy in a sense as well, since the time series are parsed by humans and thus some earthquakes are missed and as the magnitude decreases the number of missed earthquake increases, being the majority for small magnitudes. Another difficulty in the catalog is the delay that is induced by the difference of location between the detection of the earthquake and the location of our station. All the catalog has to be calibrated to ensure that the time of the earthquake corresponds to a peak in our time series. However, the speed of propagation of an earthquake isn't constant. The so-called p-waves's speed of an earthquake varies quite a bit, depending on the rocks it travels through for example.

\section{Data pre-processing}
We first have to correct the anomalies in the data: the delay induced by the physical distance separating our sensors and the seisms' location and the temporal holes.\newline

First of all, for the missing data, we take care of that when loading it. Whenever a time serie has is missing a chunk of its curve, we interpolate it with gaussian noise. This way, it will just be considered as a moment where nothing interesting happened and thus should not worsen the performance as it is quite representative to most ofthe dataset where no earthquake has happened.

Moreoever, as aformentionned, the station we choose records activity and label it with the current time which does not correspond to the exact time at which the earthquake happened according to the catalog we use. Therefore we convert latitude, longitude and elevation to a cartesian coordinate system in order to compute the distance easily. We do this using ECEF coordinates\cite{ECEFPaper}\\
Now that we have the distance, we manually find the start time of some earthquake and see the time difference. With a few time differences and distances we computed the average speed of propagation of earthquake/p-waves and got $5676.611\frac{m}{s}$ which is very close to what they found in \cite{PWavePaper}. We use that measure in order to automatically label the data.

\section{Labeling of the Data}
Since the dataset lives in a continuous space, we have to discretize it. We have to choose time windows that will be classified as earthquake or not earthquake. The choice of the length of the time window is crucial and it is hard to predict what would be a good choice. Also, that gives rise to a fundamental question for the creation of the labels: when does an earthquake end? It is possible, in fact rather probable depending on the length of the discretization process, that an earthquake spans over multiple time windows thus knowing the duration of the earthquake would permit to overcome this issue. This is important, because not being able to accurately and correctly label our data will of course be disastrous for the machine learning model. Unfortunately, the duration of an earthquake is an open question in geology, the only option simple enough is to label as earthquake only the window that contains the moment of an earthquake.\\
The problem with this approach is illustrated in figure \ref{fig:problem-discretization}. In the catalog the earthquake starts at $T_1$ and we can see that the earthquake indeed triggers a big instant augmentation in the variation of the amplitude, that is typical to the earthquakes that are quite easy to classify. The problem here is that when discretizing the time series in windows, we will likely not have a window starting at $T_1$. Let's say that our partitioning gives a time window that ends at $T_2$. Therefore, we would label the time window containing the start of the earthquake, i.e $T_1$, as an earthquake and hence the one starting at $T_2$ as noise. However, the latter is the one containing most of the earthquake so our labels would most likely yield a time window with activated features whereas it does only contain the very start of an earthquake and another one with unactivated features for a time window that indeed contains an earthquake.

\begin{figure}[h]
  \centering
	\includegraphics[width=\columnwidth]{problem-time-window-labeling.png}
  \caption{Illustration of the problem of time window labeling}
	\label{fig:problem-discretization}
\end{figure}

\section{Feature Creation}
From that seismic waves we have to choose what processing and transformations will have to be done to be able to feed the machine learning algorithm. We have two main options: leave it as it is and opt for an algorithm such as convolutional neural networks, the approach they took in ConvNetQuake (the paper \cite{ConvNetPaper} that inspired this project), or perform some feature engineering by hand to be able to use some more standard algorithm, which is the approach we chose. So, rather than having a gigantic input dimension we choose to compress it in a smart way, creating meaningful features for the algorithm. For instance, if we took a time window of $10$ seconds, this compression is on the order of $100$ if we create ten features. This permits to accelerate the training process and thus allow to span more time or take more channel.\newline

There are a few characteristics inherent to earthquakes that we try to capture through our features: they have high frequencies, high amplitudes, quick acceleration but rather slow decrease and a lot of different frequencies. We begin by adding simple measures that directly come to mind: the standard deviation, the maximum and minimum amplitude \textbf{ask lucien}. For the next ones, let's take an example to see what characterise an earthquake:\newline
- Examples and presentation of the two first features
- Transition to fourier and presentation of those features

\section{Model Selection}
- Tried svm with gaussian kernel $\rightarrow$ shitty\\
- Elaborate on the unbalance problem $\rightarrow$ trees\\
- Difficult so we move to a simpler problem $\rightarrow$ present richter's magnitude scale\\
- Still difficult\\

\section{Conclusion}
That project did not seem hard at first, but we went through many difficulties. First, due to the location dependency and missing values in the signal which we had to manually solve, since ObsPy does not provide a solution. Secondly, because of the fact that earthquakes in our data do not always trigger features that we expect since signals from stations are sometimes very noisy. Additionally, automatically splitting the original time serie yields chunks that can be mislabled because we only know the start of a seism and since our dataset has a strong uneven proportion of earthquake time windows compared to noisy ones, things can quickly go awry. Then, the feature engineering was not trivial and it took some time to come up with good approaches that capture the essence of earthquakes.\newline
Thus, after all the pre-processing and feature engineering we did not have that much time left to devote to the training and model choosing we did not manage to train a good model for the following reasons: our features did not activate perfectly, our labels were hugely imbalanced and some windows were mislabled due to the time window problems. Our first goal was to train a model that detects any earthquake whatever their magnitude. However, small earthquakes did not activate our features enough, partly because of the noise, so we decided to focus on those whose magnitude is more than $3.5$ on the Ricther scale. Given the imbalanceness of the two classes we focused on a high recall finish bob when results \newline


\section{Further improvements}
There are a few things that we could improve in order to have a better model. The first one is simply better features. Indeed, with better knowledge in geology and more precisely seismic waves, one could come up with features that captures more how earthquakes behave in a signal. Secondly, one could try to come up with a better way to choose time windows that could try to take into account where a seism starts so that it does includes at least a certain percentage of what comes after. Then, in order to have more diversity in our dataset, we could have made use of more than one station.

- Better features, signals, seismology\\ done 
- Undersampling\\
- Oversampling\\
- More robust way to compute speed of P-waves, for example compute average speed in each different direction or get ground information to have a better idea of how fast p-waves will go through the ground\\
- Finding a better way to decide if a time-window has an earthquake or not\\ sort of done
- Multiple stations\\ done
- Better way to choose time window?\\ done
- Better knowledge of deep nets or random forests
- Try another place with more earthquake as we focus on notiecable earthquake and those are rare in oklahoma

\begin{thebibliography}{9}
\bibitem{ConvNetPaper}
Convolutional neural network for earthquake detection and location, 14 February 2018. \\\texttt{https://advances.sciencemag.org/content/4/2/e1700578}
\bibitem{obspy}
ObsPy: python library to collect seismological data.
\\\texttt{https://docs.obspy.org/}
\bibitem{ECEFPaper}
J. Zhu, (1994),  Conversion of Earth-centered Earth-fixed coordinates to geodetic coordinates. \\\texttt{https://ieeexplore.ieee.org/abstract/document/303772/}
\bibitem{PWavePaper}
Guy T. Kuster and M. Nafi ToksÃ¶z, (1974), "Velocity and Attenuation of Seismic Waves in twoâphase Media: Part I. Theoretical Formulations," geophysics 39: 587-606. \\\texttt{https://library.seg.org/doi/abs/10.1190/1.1440450}
\end{thebibliography}

\end{document}
